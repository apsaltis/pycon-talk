<[autotemplate]
documentclass={beamer}
usepackage={listings}
usepackage={wasysym}
usepackage={graphicx}
date={\today}
lstdefinestyle={basic}{....}
titleframe=True
title={Scrape the Web}
subtitle={Strategies for programming websites that don't expect it}
author={Asheesh Laroia asheesh@asheesh.org}
[autotemplate]>

==== This is supposed to be practice ====

* <1->But there are things I know I'll have to fix
* <2->More graphs and diagrams
** <3->But which ones?
* <4->Better presentation style (sorry!)

==== So please do interrupt me ====

==== ...and we begin ====

==== Welcome ====

==== Meta ====

*<1->You will learn neat tricks
*<2->DO NOT BECOME AN EVIL COMMENT SPAMMER
*<3->Theory and running code
*<4->Brittle? Sometimes.
*<5->The comics aren't mine; ask for references.

==== Things you'll need ====

* Sample code: http://FIXME.com
* Install FireBug
* Install python-lxml if it is easy

==== Pacing ====

* <1->Slow me down,
* <2->or speed me up.
* <2->With your voice, or by raising your hand.

==== What is web scraping? ====

Generally speaking,

* <1->You retrieve some data from the web,
* <2->You extract some information,
* <3->and optionally you repeat.

==== Perspectives on scraping ====

* <1->One page vs. a whole site
* <2->A site's contents now, or for the future as well

====  ====

(1) Diving in with curry

==== ====

<<<curry.jpg,width=4.5in>>>

==== Lunchtime ====

* <1-> http://mehfilindian.com/LunchMenuTakeOut.htm
* <2-> A question
** <3-> is there eggplant today?

==== From python ====

examples/curry/trivial.py

==== ====

(2) HTML: Structured text on the web

==== Two easy ways to read HTML ====

In a browser:

* <1->View source
* <2->Inspect element (requires Firebug or DOM Inspector)

==== HTML vs. XHTML ====

* <1-> Both are trees of tags
* <2-> HTML: from 1992
* <3-> XHTML: from 2000
* <4-> ...did XHTML win?

==== Stats pop quiz: size and type ====

(Stats from the MAMA survey published by Opera,
http://dev.opera.com/articles/view/mama-key-findings/.)

* <1->Average page size?
** <2-> 16.5K
*<3-> HTML to XHTML ratio?
**<4-> 2:1

==== Stats pop quiz: quirks and tags (pt. 1) ====

* <1->Transitional vs. Strict+Framset?
** <2->10:1
* <3->How many pages render in "Quirks" mode?
** <4->85 percent
* <5->What percent validate?
** <6->4.13 percent

==== Stats pop quiz: quirks and tags (pt. 2) ====

* <1->What's more popular? TITLE or BODY?
** <2->TITLE
* <3->What percent of web pages with validations badges actually validate?
** <4-> about 50 percent
* The web is a mess

==== ====

(3) Parsing "HTML" in Python

==== Things to consider ====

*<1-> What does a parser do with invalid HTML?
*<2-> Does it handle XHTML properly?
**<3-> They all do; don't worry.
*<4-> examples/parsing/ has samples.

==== A showcase of some options ====

* HTMLParser (stdlib!)
* xml.dom.minidom (stdlib!)
* <2->BeautifulSoup
* <2->html5lib
* <2->lxml.html

==== ====

(4) On regular expressions

==== ====

* Some people, when confronted with a problem, think
** "I know, I'll use regular expressions."
* Now they have two problems. -- jwz.

==== But why? ====

* a href="whatever"
* a href='whatever'
* a href="whatever'

==== But it's good enough for... ====

*<1->Curry
*<2->Text analysis:
** <3->Reviews 1-10 of 430

==== If you have to ====

* Use Kodos, a regular expression GUI
* (Note: redemo.py in Python source is unmaintained.)
* <2-> Be conservative in what you do, be liberal in what you accept from others. -- Jon Postel.

==== ====

(5) Parsers in depth

==== Searching document trees ====

* BeautifulSoup API (examples/tree-builders/beautifulsoup/search.py)
* <2->html5lib creates BeautifulSoup objects (or others) (examples/tree-builders/html5lib/search.py)
* <3->lxml provides XPath (examples/tree-builders/lxml/search xpath.py)
* <4->lxml provides CSSSelect (examples/tree-builders/lxml/search css.py)

==== General structure of scraping a page ====

* Get a page's HTML
* <2->Parse it
* <3->Pull out the items you need
* <4->Return them as a dictionary, or an object

==== A closer look at curry ====

* (see Python interpreter)
* (let's use BeautifulSoup)
* <2->Conclusion: This is a text-processing problem, not a tag problem.

==== Mini-lesson ====

Three kinds of page:

* Hand-written pages
* <2->Machine-written pages
* <3->Machine-written pages, old-skool

==== More BeautifulSoup: Yahoo! Finance ====

* examples/tree-builders/beautifulsoup yfinance.py

==== ====

(6) Interacting with the web

==== Hard-coding URLs: Yahoo! search ====

* examples/search/yahoo.py
* examples/search/google.py

==== More about HTTP: Headers ====

* (Firefox demo)

==== More about HTTP: Status codes ====

* 2xx: Success
* 3xx: Redirection
* 4xx: Error
* <2->402: Payment Required
* <2->404 Not Found
* <3->410 Gone
* <4->418 I'm a teapot

==== More about HTTP: Methods ====

* GET
* POST
* PUT
* <2->BREW

==== Once we set User-Agent, are we just like Firefox? ====

* JavaScript behavior
* <2->Image download behavior
* <3->Cookie behavior
* <4->Invalid HTML handling behavior (?)
* <5->Accept: headers

==== Google, again ====

* examples/search/urllib2-user-agent/google as ie.py
* <2->IE 5 vs. IE 8

==== robots.txt ====

* User-agent: *
* Disallow: /
* Allow: /crawlme.html
* <2->http://www.robotstxt.org/
* <3->Don't ever GET it

==== ====

(7) Filling out forms, and handling cookies, with mechanize

==== The weather (by hand) ====

* http://cepstral.com/cgi-bin/demos/weather
* <2->Find the POST target in Firebug
* <3->examples/cepstral/just post.py
* <4->examples/cepstral/play wav.py

==== The weather (with mechanize) ====

* examples/cepstral/just post via mechanize.py

==== Search the web (via mechanize) ====

* examples/search/yahoo mechanize.py
* <2->Oh snap, we're a robot.
* <3->examples/search/yahoo mechanize norobots.py
* <4->examples/search/google mechanize.py

==== ====

(8) Recap and philosophy

==== Things we've seen ====

* Loading web pages from the network with urllib2
* <2->Parsing web pages (even broken ones)
* <3->Scraping that page into a set of structured Python objects
* <4->HTTP status codes
* <5->Faking the user agent header
* <6->Submitting forms
* <7->Keeping a session with cookies

==== ====

(9) Even more about parsers

==== Things to look for ====

* Performance
* Ease-of-use
* Quality
* Maintained-ness

==== Redux ====

* <1>HTMLParser (stdlib!)
* <2->xml.dom.minidom (stdlib!)
* <3->BeautifulSoup
* <4->html5lib
* <5->lxml.html

==== Winners ====

* Resilience: lxml.html == html5lib > BeautifulSoup > stdlib
* <2->Performance: lxml.html > stdlib > BeautifulSoup > html5lib

==== ====

(10) Countermeasures

==== The basics ====

* Referer header
* Cookies
* Hidden form fields
* <2->Solved by mechanize

==== The hard ones ====

* Per-IP address query limits
* <2->Behavior profiling
* <3->JavaScript
* <4-> CAPTCHAs

==== IP addresss ====

* ssh -D
* tsocks
* socks monkey
* <2> (All in the sample code)

==== Behavior profiling ====

* You're doomed.

==== JavaScript ====

Options:

# Re-write the JS in Python
# Send the JS to python-spidermonkey

==== CAPTCHAs ====

* Hope for an easy one
** http://www.mailinator.com/images/captcha1.gif
* <2->Otherwise, just present it to the human operator...

==== ====

(11) When (and how) to just automate Firefox

==== Selenium and friends ====

* examples/seleniumrc/test google.py
* Selenium RC for XPath

==== Thanks ====

asheesh@asheesh.org

