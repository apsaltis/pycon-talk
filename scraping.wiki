<[autotemplate]
documentclass={beamer}
usepackage={listings}
usepackage={wasysym}
usepackage={graphicx}
date={\today}
lstdefinestyle={basic}{....}
titleframe=True
title={Scrape the Web}
subtitle={Strategies for programming websites that don't expect it}
author={Asheesh Laroia asheesh@asheesh.org}
[autotemplate]>

==== Welcome ====

==== Meta ====

*<1->You will learn neat tricks
*<2->DO NOT BECOME AN EVIL COMMENT SPAMMER
*<3->Theory and running code
*<4->Brittle? Sometimes.

==== Things you'll need ====

* Sample code: http://FIXME.com
* Install FireBug or Chrome DOM Inspector
* Install these Python modules, if easy:
** lxml (apt-get install python-lxml)
** requests
** mechanize
** BeautifulSoup

==== Pacing ====

* <1->Slow me down,
* <2->or speed me up.
* <2->With your voice, or by raising your hand.

==== What is web scraping? ====

Categories:

* <1->one page that you need to process.
* <2->an entire website you have to crawl and harvest data from.
* <3->a set of forms that give you access to a web service.
* <4->any of the above, in an ongoing fashion.

====  ====

(1) The website is the API

==== ====

http://cepstral.com/cgi-bin/demos/weather

==== via mechanize ====

* <1->examples/cepstral/just post via mechanize.py
* <2->Drat, we get HTML.

==== For now, forget HTML ====

* <1->examples/cepstral/stupid string parse.py
* <2->examples/cepstral/all together.py
* <3->Yay!

==== ====

<<<curry.jpg,width=4.5in>>>

==== Lunchtime ====

* <1->http://mehfilindian.com/LunchMenuTakeOut.htm
* <2->examples/curry/trivial.py

==== Un-answered questions ====

* <1->Will it break?
* <2->What is a non-stupid way to parse HTML?
* <3->What is mechanize doing?
* <4->Does development have to be so slow?

==== ====

(2) HTML: Structured text on the web

==== Two easy ways to read HTML ====

In a browser:

* <1->View source
* <2->Inspect element (requires Firebug or DOM Inspector)

==== HTML vs. XHTML ====

* <1-> Both are trees of tags
* <2-> HTML: from 1992
* <3-> XHTML: from 2000
* <4-> ...did XHTML win?

==== Stats on the web ====

(Stats from the MAMA survey published by Opera,
http://dev.opera.com/articles/view/mama-key-findings/.)

* <1->Average page size?
** <2-> 16.5K
*<3-> HTML to XHTML ratio?
**<4-> 2:1
* <5->How many pages render in "Quirks" mode?
** <6->85 percent

==== Stats pop quiz: quirks and tags (pt. 1) ====

* <1->What percent validate?
** <2->4.13 percent
* <3->What percent of web pages with validation badges actually validate?
** <4-> about 50 percent
* <5->What's more popular? TITLE or BODY?
** <6->TITLE
* The web is a mess

==== ====

(3) On regular expressions

==== ====

* Some people, when confronted with a problem, think
** "I know, I'll use regular expressions."
* Now they have two problems. -- jwz.

==== But why? ====

* a href="whatever"
* a href='whatever'
* a href="whatever'

==== But it's good enough for... ====

*<1->Trivial tasks (e.g., "is there eggplant?")
*<2->Some automatically generated HTML (e.g., "Find the .wav file")
*<3->Text analysis:
** <4->Reviews 1-10 of 430

==== If you have to ====

* Use Kodos, a regular expression GUI
* (Note: redemo.py in Python source is unmaintained.)
* <2-> Be conservative in what you do, be liberal in what you accept from others. -- Jon Postel.

==== ====

(4) Traversing parsed HTML with CSS selectors

==== FIXME ====

easy: lxml.html

==== Do it for cepstral ====

* FIXME

==== Common things for text in HTML ====

* "Remove all tags"
* Convert entities into Unicode strings
* FIXME

==== ====
(5) More about HTTP

==== More about HTTP: Headers ====

* (Firefox demo)

==== More about HTTP: Status codes ====

* 2xx: Success
* 3xx: Redirection
* 4xx: Error
* <2->402: Payment Required
* <2->404 Not Found
* <3->410 Gone
* <4->418 I'm a teapot

==== More about HTTP: Methods ====

* GET
* POST
* PUT
* <2->BREW

==== urllib2 vs. mechanize vs. requests ====

* <1->Try to POST to: http://www.cepstral.com/cgi-bin/demos/weather
* <2->Contrast code samples
* <3->Odd tip: Sometimes GET == POST

==== ====

(6) On bots and their detection

==== robots.txt ====

* User-agent: *
* Disallow: /
* Allow: /crawlme.html
* <2->http://www.robotstxt.org/
* <3->Don't ever GET it

==== Search the web (via mechanize) ====

* examples/search/yahoo mechanize.py
* <2->Oh snap, we're a robot.
* <3->examples/search/yahoo mechanize norobots.py

==== Hard-coding URLs: Yahoo! search ====

* examples/search/yahoo.py
* examples/search/google.py

==== Google, again ====

* <1->examples/search/google mechanize.py
* <2->examples/search/urllib2-user-agent/google as ie.py
* <3->IE 5 vs. IE 8

==== ====

(7) More about HTML parsing

==== Things to consider ====

*<1-> What does a parser do with invalid HTML?
*<2-> Does it handle XHTML properly?
**<3-> They all do; don't worry.
*<4-> examples/parsing/ has samples.

==== A showcase of some options ====

* HTMLParser (stdlib!)
* xml.dom.minidom (stdlib!)
* <2->BeautifulSoup
* <2->html5lib
* <2->lxml.html

==== Searching document trees ====

* BeautifulSoup API (examples/tree-builders/beautifulsoup/search.py)
* <2->html5lib creates BeautifulSoup objects (or others) (examples/tree-builders/html5lib/search.py)
* <3->lxml provides XPath (examples/tree-builders/lxml/search xpath.py)
* <4->lxml provides CSSSelect (examples/tree-builders/lxml/search css.py)

==== General structure of scraping a page ====

* Get a page's HTML
* <2->Parse it
* <3->Pull out the items you need
* <4->Return them as a dictionary, or an object

==== A closer look at curry ====

* (see Python interpreter)
* (let's use BeautifulSoup)
* <2->Conclusion: This is a text-processing problem, not a tag problem.

==== More BeautifulSoup: Yahoo! Finance ====

* examples/tree-builders/beautifulsoup yfinance.py

==== ====

(7) In short: How cookies work

* FIXME
* Requests and mechanize handle it for you

==== ====

(8) Recap and philosophy

==== Things we've seen ====

* Loading web pages from the network with urllib2
* <2->Parsing web pages (even broken ones)
* <3->Scraping that page into a set of structured Python objects
* <4->HTTP status codes
* <5->Faking the user agent header
* <6->Submitting forms
* <7->Keeping a session with cookies

==== ====

(9) Even more about parsers

==== Things to look for ====

* Performance
* Ease-of-use
* Quality
* Maintained-ness

==== Redux ====

* <1>HTMLParser (stdlib!)
* <2->xml.dom.minidom (stdlib!)
* <3->BeautifulSoup
* <4->html5lib
* <5->lxml.html

==== Winners ====

* Resilience: lxml.html == html5lib > BeautifulSoup > stdlib
* <2->Performance: lxml.html > stdlib > BeautifulSoup > html5lib

==== ====

(10) Countermeasures

==== Once we set User-Agent, are we just like Firefox? ====

* Accept: headers, and other HTTP headers
* JavaScript behavior
* Image + CSS download behavior
* Cookie behavior
* Invalid HTML handling behavior (?)

==== The basics ====

* Referer header
* Cookies
* Hidden form fields
* <2->Solved by mechanize

==== The hard ones ====

* Per-IP address query limits
* <2->Behavior profiling
* <3->JavaScript
* <4-> CAPTCHAs

==== IP addresss ====

* ssh -D
* tsocks
* socks monkey
* <2> (All in the sample code)

==== Behavior profiling ====

* You're doomed.

==== JavaScript ====

Options:

# Re-write the JS in Python
# Send the JS to python-spidermonkey

==== CAPTCHAs ====

* Hope for an easy one
** http://www.mailinator.com/images/captcha1.gif
* <2->Otherwise, just present it to the human operator...

==== ====

(11) When (and how) to just automate Firefox

==== Selenium and friends ====

* examples/seleniumrc/test google.py
* Selenium RC for XPath

==== Thanks ====

asheesh@asheesh.org

